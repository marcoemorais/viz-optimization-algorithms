{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "The Broyden-Fletcher-Goldfarb-Shanno aka BFGS method is a second-order iterative optimization method.\n",
    "\n",
    "### Quasi-Newton Methods\n",
    "The BFGS method is referred to as Quasi-Newton in reference to the fact that unlike Newton's method which uses an explicit Hessian matrix, these methods approximate the Hessian. \n",
    "\n",
    "$$\n",
    "x_{k+1} = x_k - \\alpha_k B_k^{-1} \\nabla f(x_k)\n",
    "$$\n",
    "\n",
    "where\n",
    "* $B_k$ is approxmation to Hessian\n",
    "* $\\alpha_k$ is obtained from line search\n",
    "\n",
    "Secant updating methods have superlinear convergence ($1 < r < 2$).\n",
    "* Slower to converge than Newton's method, but cost-per-iteration is less.\n",
    "\n",
    "### BFGS Algorithm\n",
    "1. Start with some initial guess $x_0$ and approximate Hessian $B_0 = I$.\n",
    "2. Solve $B_k s_k = -\\nabla f(x_k)$ for $s_k$ or use a line search (described below) to find $s_k$.\n",
    "3. Compute $x_{k+1} = x_k + s_k$.\n",
    "4. Compute the difference in gradients $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$.\n",
    "5. Update approximate Hessian.\n",
    "$$\n",
    "B_{k+1} = B_k + \\frac{y_k y_k^T}{y_k^T s_k} - \\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k}\n",
    "$$\n",
    "6. Repeat from step 2 until some stopping criteria is reached.\n",
    "\n",
    "### References\n",
    "> Michael T. Heath. 2018. Scientific Computing: An Introductory Survey, Revised Second Edition. SIAM-Society for Industrial and Applied Mathematics, Philadelphia, PA, USA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The numpy interface of autograd wraps all numpy ops with autodiff.\n",
    "import autograd.numpy as np\n",
    "\n",
    "from autograd import grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A line search is used to find the distance along the descent direction of the next step of the optimization.  The scalar multiple $\\alpha$ along the descent direction $d$ is found by minimizing the function below.\n",
    "\n",
    "$$\n",
    "\\underset{\\alpha}{\\text{minimize}} f(x_k + \\alpha d)\n",
    "$$\n",
    "where\n",
    "* $f(...)$ is the function to minimize\n",
    "* $x_k$ is the current solution\n",
    "* $\\alpha$ is a scalar \n",
    "* $d$ is a vector that describes the descent direction of the function\n",
    "\n",
    "For first-order optimization problems the descent direction is given by the negative gradient $-\\nabla f(x_k)$.\n",
    "\n",
    "For second-order optimization problems the descent direction is given by by the product of the negative gradient and Hessian $-\\nabla f(x_k) H_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bracket_minimum(fx, x, s, k):\n",
    "    \"\"\"\n",
    "    bracket_minimum returns interval [a,b] that brackets mininum of fx\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fx : function\n",
    "        function\n",
    "    x : numpy.ndarray\n",
    "        starting position around which bracket is found\n",
    "    s : numpy.ndarray\n",
    "        initial step size separating [a,b]\n",
    "    k : float\n",
    "        scaling factor applied to step size at each iteration\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        lower bound of bracket interval\n",
    "    numpy.ndarray\n",
    "        upper bound of bracket interval\n",
    "    \"\"\"\n",
    "    a, fxa = x, fx(x)\n",
    "    b, fxb = x + s, fx(x + s)\n",
    "    if fxb > fxa:  # Invariant: a < b.\n",
    "        a, b = b, a\n",
    "        fxa, fxb = fxb, fxa\n",
    "        s = -s\n",
    "    while True:\n",
    "        c, fxc = b + s, fx(b + s)\n",
    "        if fxc > fxb:\n",
    "            break\n",
    "        a, fxa, b, fxb = b, fxb, c, fxc\n",
    "        s = s * k\n",
    "    if a < c:\n",
    "        return a, c\n",
    "    return c, a\n",
    "\n",
    "\n",
    "def goldensection(fx, a, b, tol):\n",
    "    \"\"\"\n",
    "    goldensection returns the minimum of fx over some interval [a,b]\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fx : function\n",
    "        function\n",
    "    a : numpy.ndarray\n",
    "        lower bound of interval that brackets minimum of fx\n",
    "    b : numpy.ndarray\n",
    "        upper bound of interval that brackets minimum of fx\n",
    "    tol : float\n",
    "        convergence threshold\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        point along interval [a,b] where fx is minimum\n",
    "    \"\"\"\n",
    "    tau = (np.sqrt(5) - 1.) / 2.  # Golden ratio - 1.\n",
    "    x1, x2 = a + (1. - tau) * (b - a), a + tau * (b - a)\n",
    "    fx1, fx2 = fx(x1), fx(x2)\n",
    "    \n",
    "    while (b - a) > tol:\n",
    "        if fx1 < fx2:\n",
    "            b = x2\n",
    "            # Treat x1 as the new x2.\n",
    "            x2, fx2 = x1, fx1\n",
    "            # Compute new x1.\n",
    "            x1 = a + (1. - tau) * (b - a)\n",
    "            fx1 = fx(x1)\n",
    "        else:\n",
    "            a = x1\n",
    "            # Treat x2 as the new x1.\n",
    "            x1, fx1 = x2, fx2\n",
    "            # Compute new x2.\n",
    "            x2 = a + tau * (b - a)\n",
    "            fx2 = fx(x2)\n",
    "    \n",
    "    return x1\n",
    "\n",
    "\n",
    "def line_search(fx, d, xk, tol):\n",
    "    \"\"\"\n",
    "    line_search returns the offset from xk where fx is minimum\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fx : function\n",
    "        function\n",
    "    d : numpy.ndarray\n",
    "        descent direction of fx, typically negative gradient at xk\n",
    "    xk : numpy.ndarray\n",
    "        starting position of search\n",
    "    tol : float\n",
    "        convergence threshold\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    alpha : float\n",
    "        scalar multiple along descent direction where fx is minimum\n",
    "    numpy.ndarray\n",
    "        position xk where fx is minimum\n",
    "    \"\"\"\n",
    "\n",
    "    # Objective function to minimize.\n",
    "    fobj = lambda alpha: fx(xk + alpha * d)\n",
    "    alpha0 = 1e-6\n",
    "\n",
    "    # Find interval [a,b] closest to alpha0 that brackets the minimum.\n",
    "    a, b = bracket_minimum(fobj, alpha0, s=1e-2, k=2.)\n",
    "\n",
    "    # Find minimum within the bracket [a,b].\n",
    "    alphak = goldensection(fobj, a, b, tol)\n",
    "\n",
    "    # Position where fx is minimum.\n",
    "    xkmin  = xk + alphak * d\n",
    "    \n",
    "    return alphak, xkmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BFGS Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfgs(fx, gradfx, x0, tol, maxiter):\n",
    "    \"\"\"\n",
    "    bfgs returns the point xk where fx is minimum\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fx : function\n",
    "        function to minimize\n",
    "    gradfx : function\n",
    "        gradient of function to minimize\n",
    "    x0 : numpy.ndarray\n",
    "        initial guess for xk\n",
    "    tol : float\n",
    "        convergence threshold\n",
    "    maxiter : int\n",
    "        maximum number of iterations\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        vector xk where fx is minimum\n",
    "    numpy.ndarray\n",
    "        position and value history\n",
    "        [[x0, fx(x0), gradfx(x0)],\n",
    "         [x1, fx(x1), gradfx(x1)],...]\n",
    "    \"\"\"\n",
    "\n",
    "    xk, gradfxk, Bk = x0, gradfx(x0), np.eye(x0.size)\n",
    "\n",
    "    # Save current and minimum position and value to history.\n",
    "    steps = np.zeros((maxiter, (x0.size*2)+1))\n",
    "    steps[0,:] = np.hstack((x0, fx(x0), gradfxk))\n",
    "\n",
    "    # Repeat up to maximum number of iterations.\n",
    "    for k in range(1,maxiter):\n",
    "\n",
    "        # Stop iteration when gradient is near zero.\n",
    "        if np.linalg.norm(gradfxk) < tol:\n",
    "            steps = steps[:-(maxiter-k),:]\n",
    "            break\n",
    "\n",
    "        # Solve Bk*sk = -grad(xk) for sk.\n",
    "        try:\n",
    "            sk = np.linalg.solve(Bk, -1. * gradfxk)\n",
    "        except np.linalg.LinAlgError:\n",
    "            # Perform line search to find alphak.\n",
    "            normd = -np.dot(Bk, gradfxk)\n",
    "            normd = normd / np.linalg.norm(normd)\n",
    "            alphak, xkp1 = line_search(fx, normd, xk, tol)\n",
    "            sk = xkp1 - xk\n",
    "\n",
    "        # Update xk and evaluate gradient at new value of xk.\n",
    "        xk = xk + sk\n",
    "        gradfxk1 = gradfx(xk)\n",
    "\n",
    "        # Compute difference in gradients.\n",
    "        yk = gradfxk1 - gradfxk\n",
    "\n",
    "        # Update approximate Hessian.\n",
    "        term1 = np.outer(yk, yk.T) / np.dot(yk.T, sk)\n",
    "        term2a = np.dot(np.dot(Bk, np.outer(sk, sk.T)), Bk)\n",
    "        term2b = np.dot(np.dot(sk.T, Bk), sk)\n",
    "        Bk = Bk + term1 - (term2a / term2b)\n",
    "\n",
    "        # Update the gradient at xk.\n",
    "        gradfxk = gradfxk1\n",
    "\n",
    "        # Save iteration history.\n",
    "        steps[k,:] = np.hstack((xk, fx(xk), gradfxk))\n",
    "\n",
    "    return xk, steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Function: Rosenbrock Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x):\n",
    "    \"\"\"\n",
    "    rosenbrock evaluates Rosenbrock function at vector x\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array\n",
    "        x is a D-dimensional vector, [x1, x2, ..., xD]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        scalar result\n",
    "    \"\"\"\n",
    "    D = len(x)\n",
    "    i, iplus1 = np.arange(0,D-1), np.arange(1,D)\n",
    "    return np.sum(100*(x[iplus1] - x[i]**2)**2 + (1-x[i])**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to Rosenbrock Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0               : [-1. -1.]\n",
      "rosenbrock f(w0) : 404.0\n",
      "----------------------------------\n",
      "xk               : [0.9999072  0.99982128]\n",
      "rosenbrock f(xk) : 1.3326100865859894e-08\n",
      "nsteps           : 121\n",
      "norm(gradfx)     : 0.0032376948029769173\n"
     ]
    }
   ],
   "source": [
    "fx, gradfx = rosenbrock, grad(rosenbrock)\n",
    "x0, tol, maxiter = np.array([-1.,-1.]), 1e-2, 1000\n",
    "xk, steps = bfgs(fx, gradfx, x0, tol, maxiter)\n",
    "\n",
    "print(\"x0               :\", x0)\n",
    "print(\"rosenbrock f(w0) :\", rosenbrock(x0))\n",
    "print(\"----------------------------------\")\n",
    "print(\"xk               :\", xk)\n",
    "print(\"rosenbrock f(xk) :\", rosenbrock(xk))\n",
    "print(\"nsteps           :\", len(steps))\n",
    "print(\"norm(gradfx)     :\", np.linalg.norm(steps[-1,3:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Function: Goldstein-Price Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def goldstein_price(x):\n",
    "    \"\"\"\n",
    "    goldstein_price evaluates Goldstein-Price function at vector x\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array\n",
    "        x is a 2-dimensional vector, [x1, x2]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        scalar result\n",
    "    \"\"\"\n",
    "    a = (x[0] + x[1] + 1)**2\n",
    "    b = 19 - 14*x[0] + 3*x[0]**2 - 14*x[1] + 6*x[0]*x[1] + 3*x[1]**2\n",
    "    c = (2*x[0] - 3*x[1])**2\n",
    "    d = 18 - 32*x[0] + 12*x[0]**2 + 48*x[1] - 36*x[0]*x[1] + 27*x[1]**2\n",
    "    return (1. + a*b) * (30. + c*d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to Goldstein-Price Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0                    : [-1.  -1.5]\n",
      "goldstein_price f(w0) : 1595.41015625\n",
      "----------------------------------\n",
      "xk                    : [ 1.40617001e-05 -9.99995623e-01]\n",
      "goldstein_price f(xk) : 3.000000044809572\n",
      "nsteps                : 95\n",
      "norm(gradfx)          : 0.006186613015289871\n"
     ]
    }
   ],
   "source": [
    "fx, gradfx = goldstein_price, grad(goldstein_price)\n",
    "x0, tol, maxiter = np.array([-1.0,-1.5]), 1e-2, 1000\n",
    "xk, steps = bfgs(fx, gradfx, x0, tol, maxiter)\n",
    "\n",
    "print(\"x0                    :\", x0)\n",
    "print(\"goldstein_price f(w0) :\", goldstein_price(x0))\n",
    "print(\"----------------------------------\")\n",
    "print(\"xk                    :\", xk)\n",
    "print(\"goldstein_price f(xk) :\", goldstein_price(xk))\n",
    "print(\"nsteps                :\", len(steps))\n",
    "print(\"norm(gradfx)          :\", np.linalg.norm(steps[-1,3:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

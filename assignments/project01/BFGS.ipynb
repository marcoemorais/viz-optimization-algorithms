{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "The Broyden-Fletcher-Goldfarb-Shanno aka BFGS method is a second-order iterative optimization method.\n",
    "\n",
    "### Quasi-Newton Methods\n",
    "The BFGS method is referred to as Quasi-Newton in reference to the fact that unlike Newton's method which uses an explicit Hessian matrix, these methods approximate the Hessian. \n",
    "\n",
    "$$\n",
    "x_{k+1} = x_k - \\alpha_k B_k^{-1} \\nabla f(x_k)\n",
    "$$\n",
    "\n",
    "where\n",
    "* $B_k$ is approxmation to Hessian\n",
    "* $\\alpha_k$ is obtained from line search\n",
    "\n",
    "Secant updating methods have superlinear convergence ($1 < r < 2$).\n",
    "* Slower to converge than Newton's method, but cost-per-iteration is less.\n",
    "\n",
    "### BFGS Algorithm\n",
    "1. Start with some initial guess $x_0$ and approximate Hessian $B_0 = I$.\n",
    "2. Solve $B_k s_k = -\\nabla f(x_k)$ for $s_k$.\n",
    "3. Compute $x_{k+1} = x_k + s_k$.\n",
    "4. Compute the difference in gradients $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$.\n",
    "5. Update approximate Hessian.\n",
    "$$\n",
    "B_{k+1} = B_k + \\frac{y_k y_k^T}{y_k^T s_k} - \\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k}\n",
    "$$\n",
    "6. Repeat from step 2 until some stopping criteria is reached.\n",
    "\n",
    "Alternate: Replace $B_k$ update with factorization to reduce $O(n^3)$ work to $O(n^2)$.\n",
    "\n",
    "### References\n",
    "> Michael T. Heath. 2018. Scientific Computing: An Introductory Survey, Revised Second Edition. SIAM-Society for Industrial and Applied Mathematics, Philadelphia, PA, USA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The numpy interface of autograd wraps all numpy ops with autodiff.\n",
    "import autograd.numpy as np\n",
    "\n",
    "from autograd import grad\n",
    "\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BFGS Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfgs(fx, gradfx, x0, tol, maxiter):\n",
    "    \"\"\"\n",
    "    bfgs returns the point xk where fx is minimum\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fx : function\n",
    "        function to minimize\n",
    "    gradfx : function\n",
    "        gradient of function to minimize\n",
    "    x0 : numpy.ndarray\n",
    "        initial guess for xk\n",
    "    tol : float\n",
    "        convergence threshold\n",
    "    maxiter : int\n",
    "        maximum number of iterations\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        vector xk where fx is minimum\n",
    "    numpy.ndarray\n",
    "        position and value history\n",
    "        [[x0, fx(x0), gradfx(x0)],\n",
    "         [x1, fx(x1), gradfx(x1)],...]\n",
    "    \"\"\"\n",
    "\n",
    "    xk, gradfxk, Bk = x0, gradfx(x0), np.eye(x0.size)\n",
    "\n",
    "    # Save current and minimum position and value to history.\n",
    "    steps = np.zeros((maxiter, (x0.size*2)+1))\n",
    "    steps[0,:] = np.hstack((x0, fx(x0), gradfxk))\n",
    "\n",
    "    # Repeat up to maximum number of iterations.\n",
    "    for k in range(1,maxiter):\n",
    "\n",
    "        # Stop iteration when gradient is near zero.\n",
    "        if np.linalg.norm(gradfxk) < tol:\n",
    "            steps = steps[:-(maxiter-k),:]\n",
    "            break\n",
    "\n",
    "        # Solve Bk*sk = -grad(xk) for sk.\n",
    "        sk = np.linalg.solve(Bk, -1. * gradfxk)\n",
    "\n",
    "        # Update xk and evaluate gradient at new value of xk.\n",
    "        xk = xk + sk\n",
    "        gradfxk1 = gradfx(xk)\n",
    "\n",
    "        # Compute difference in gradients.\n",
    "        yk = gradfxk1 - gradfxk\n",
    "\n",
    "        # Update approximate Hessian.\n",
    "        term1 = np.outer(yk, yk.T) / np.dot(yk.T, sk)\n",
    "        term2a = np.dot(np.dot(Bk, np.outer(sk, sk.T)), Bk)\n",
    "        term2b = np.dot(np.dot(sk.T, Bk), sk)\n",
    "        Bk = Bk + term1 - (term2a / term2b)\n",
    "\n",
    "        # Update the gradient at xk.\n",
    "        gradfxk = gradfxk1\n",
    "\n",
    "        # Save iteration history.\n",
    "        steps[k,:] = np.hstack((xk, fx(xk), gradfxk))\n",
    "\n",
    "    return xk, steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Function: Rosenbrock Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x):\n",
    "    \"\"\"\n",
    "    rosenbrock evaluates Rosenbrock function at vector x\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array\n",
    "        x is a D-dimensional vector, [x1, x2, ..., xD]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        scalar result\n",
    "    \"\"\"\n",
    "    D = len(x)\n",
    "    i, iplus1 = np.arange(0,D-1), np.arange(1,D)\n",
    "    return np.sum(100*(x[iplus1] - x[i]**2)**2 + (1-x[i])**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to Rosenbrock Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0               : [-1. -1.]\n",
      "rosenbrock f(w0) : 404.0\n",
      "----------------------------------\n",
      "xk               : [0.99999997 0.99999994]\n",
      "rosenbrock f(xk) : 1.3256730840753958e-15\n",
      "nsteps           : 123\n",
      "norm(gradfx)     : 9.865687600739896e-07\n"
     ]
    }
   ],
   "source": [
    "fx, gradfx = rosenbrock, grad(rosenbrock)\n",
    "x0, tol, maxiter = np.array([-1.,-1.]), 1e-6, 20000\n",
    "xk, steps = bfgs(fx, gradfx, x0, tol, maxiter)\n",
    "\n",
    "print(\"x0               :\", x0)\n",
    "print(\"rosenbrock f(w0) :\", rosenbrock(x0))\n",
    "print(\"----------------------------------\")\n",
    "print(\"xk               :\", xk)\n",
    "print(\"rosenbrock f(xk) :\", rosenbrock(xk))\n",
    "print(\"nsteps           :\", len(steps))\n",
    "print(\"norm(gradfx)     :\", np.linalg.norm(steps[-1,3:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Function: Goldstein-Price Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def goldstein_price(x):\n",
    "    \"\"\"\n",
    "    goldstein_price evaluates Goldstein-Price function at vector x\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array\n",
    "        x is a 2-dimensional vector, [x1, x2]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        scalar result\n",
    "    \"\"\"\n",
    "    a = (x[0] + x[1] + 1)**2\n",
    "    b = 19 - 14*x[0] + 3*x[0]**2 - 14*x[1] + 6*x[0]*x[1] + 3*x[1]**2\n",
    "    c = (2*x[0] - 3*x[1])**2\n",
    "    d = 18 - 32*x[0] + 12*x[0]**2 + 48*x[1] - 36*x[0]*x[1] + 27*x[1]**2\n",
    "    return (1. + a*b) * (30. + c*d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to Goldstein-Price Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0                    : [-1.  -1.5]\n",
      "goldstein_price f(w0) : 1595.41015625\n",
      "----------------------------------\n",
      "xk                    : [ 1.45153988e-14 -1.00000000e+00]\n",
      "goldstein_price f(xk) : 2.999999999999943\n",
      "nsteps                : 23\n",
      "norm(gradfx)          : 1.1705318380849725e-10\n"
     ]
    }
   ],
   "source": [
    "fx, gradfx = goldstein_price, grad(goldstein_price)\n",
    "x0, tol, maxiter = np.array([-1.0,-1.5]), 1e-15, 20000\n",
    "#xk, steps = bfgs(fx, gradfx, x0, tol, maxiter)\n",
    "\n",
    "# NOTE(mmorais): bfgs is failing on line search, use scipy instead.\n",
    "def _bfgs(fx, gradfx, x0, tol, maxiter=None):\n",
    "    res = opt.minimize(fx, x0, method='BFGS', jac=gradfx, tol=tol)\n",
    "    # Copy OptimizeResult to equivalent returned from bfgs.\n",
    "    xk = res.x\n",
    "    # Save current and minimum position and value to history.\n",
    "    steps = np.zeros((res.nit, (x0.size*2)+1))\n",
    "    steps[-1,:] = np.hstack((res.x, res.fun, res.jac))\n",
    "    return xk, steps\n",
    "\n",
    "xk, steps = _bfgs(fx, gradfx, x0, tol, maxiter)\n",
    "\n",
    "print(\"x0                    :\", x0)\n",
    "print(\"goldstein_price f(w0) :\", goldstein_price(x0))\n",
    "print(\"----------------------------------\")\n",
    "print(\"xk                    :\", xk)\n",
    "print(\"goldstein_price f(xk) :\", goldstein_price(xk))\n",
    "print(\"nsteps                :\", len(steps))\n",
    "print(\"norm(gradfx)          :\", np.linalg.norm(steps[-1,3:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

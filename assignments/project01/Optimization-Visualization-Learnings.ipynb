{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Visualization: Learnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "\n",
    "### General\n",
    "* Comparisons between methods, such as Gradient Descent and BFGS, should be done by setting each algorithm to use similar values of convergence tolerance.\n",
    "* When using gradient-based solvers, first-order methods such as GD requires more iterations to find the global minimum than second-order methods such as BFGS.\n",
    "* Stochastic-based solvers such as Simulated Annealing can find global minima despite getting caught in local minima, but gradient-based solvers such as GD cannot.  In exchange for this advantage, the stochastic-based solvers require more iterations to find global minima. \n",
    "\n",
    "### Gradient Descent (GD)\n",
    "* Choice of learning rate, $\\alpha$, has a big impact on the algorithm.\n",
    "    * Values of $\\alpha$ that are too big fail to find a solution.\n",
    "    * Values of $\\alpha$ that are too small require more iterations to find a solution.\n",
    "* Gradient descent spends more time in flat valleys than along steep canyons.\n",
    "    * An adaptive learning rate could help address this shortcoming.\n",
    "\n",
    "### BFGS\n",
    "* My own implementation of BFGS worked without issues on the Rosenbrock function, but would periodically fail on the Goldstein-Price function.\n",
    "    * The line search step fails to compute a step length when the approximate Hessian is not invertible.\n",
    "\n",
    "### Simulated Annealing\n",
    "* The more local minima in the test function, then the more iterations that are required since some time will be spent in each local minima.\n",
    "* The transition distribution and annealing schedule hyperparameters of the algorithm can be tuned to each particular test function.\n",
    "\n",
    "## Test Functions\n",
    "\n",
    "### Goldstein-Price\n",
    "* Even a sophisticated gradient-based solver such as BFGS implemented in scipy.optimize is not always able to find the global minimum for this test function.\n",
    "    * There is a local minimum at $(x_1=0.75, x_2=0.25)$ that gradient-based solvers will find depending on the choice of initial position $x_0$.\n",
    "* Gradient descent requires more iterations but is more robust at finding the global minimum, perhaps because the step size is smaller.\n",
    "    * Need to validate the hypothesis by comparing results from different initial positions.\n",
    "\n",
    "### Egg Crate\n",
    "* This is the most challenging test function of the group due to the large number of local minima.\n",
    "\n",
    "## Visualization\n",
    "* Filled contour plots are superior for visualizating 2d surfaces.\n",
    "* Use log scale for surfaces with orders of magnitude differences in range.\n",
    "* Less effort is required to make 3d surface plots look good (for simple surfaces) with matplotlib in comparison to pyvista.\n",
    "* pyvista expects z scale to be normalized.\n",
    "* pyvista has a nice feature to add semi-transparent contour lines to a surface plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

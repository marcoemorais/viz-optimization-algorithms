{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "The Gradient Descent method is a first-order iterative optimization algorithm for finding the local minimum of a differentiable function.\n",
    "\n",
    "### Gradient Descent Algorithm\n",
    "1. Start with some initial guess $x_0$ and learning rate $\\alpha$.\n",
    "2. Update $x_k$ in the direction of negative gradient $x_k = x_k - \\alpha \\nabla f(x_{k-1})$.\n",
    "3. Evaluate the gradient at the new minimum $\\nabla f(x_k)$\n",
    "4. Repeat from step 2 until $\\nabla f(x_k) \\approx 0$\n",
    "\n",
    "### References\n",
    "> Mykel J. Kochenderfer and Tim A. Wheeler. 2019. Algorithms for Optimization. The MIT Press.\n",
    "\n",
    "[Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The numpy interface of autograd wraps all numpy ops with autodiff.\n",
    "import autograd.numpy as np\n",
    "\n",
    "from autograd import grad\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(fx, gradfx, x0, alpha, tol, maxiter=None):\n",
    "    \"\"\"\n",
    "    gradient_descent returns the point xk where fx is minimum\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fx : function\n",
    "        function to minimize\n",
    "    gradfx : function\n",
    "        gradient of function to minimize\n",
    "    x0 : numpy.ndarray\n",
    "        initial guess for xk\n",
    "    alpha : float\n",
    "        learning rate\n",
    "    tol : float\n",
    "        convergence threshold\n",
    "    maxiter : int\n",
    "        maximum number of iterations\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        point xk where fx is minimum\n",
    "    list\n",
    "        list containing points [(x0, fx(x0), gradfx(x0)), ...])\n",
    "    \"\"\"\n",
    "\n",
    "    xk, fxk, gradfxk = x0, fx(x0), gradfx(x0)\n",
    "    steps = [(x0, fxk, gradfxk)]\n",
    "    \n",
    "    # Stop iteration when gradient is near zero.\n",
    "    while np.linalg.norm(gradfxk) > tol:\n",
    "\n",
    "        # Update xk based on product of learning rate and gradient.\n",
    "        xk = xk - alpha * gradfxk\n",
    "\n",
    "        # Evaluate gradient at new value of xk.\n",
    "        gradfxk = gradfx(xk)\n",
    "\n",
    "        # Evaluate the function at new value of xk.\n",
    "        fxk = fx(xk)\n",
    "\n",
    "        # Append (xk, fxk, gradfxk) to iteration history.\n",
    "        steps.append((xk, fxk, gradfxk))\n",
    "\n",
    "        # Check early termination criteria.\n",
    "        if maxiter is not None and len(steps) == maxiter:\n",
    "            break\n",
    "\n",
    "    return xk, steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Function: Rosenbrock Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x):\n",
    "    \"\"\"\n",
    "    rosenbrock evaluates Rosenbrock function at vector x\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array\n",
    "        x is a D-dimensional vector, [x1, x2, ..., xD]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        scalar result\n",
    "    \"\"\"\n",
    "    D = len(x)\n",
    "    i, iplus1 = np.arange(0,D-1), np.arange(1,D)\n",
    "    return np.sum(100*(x[iplus1] - x[i]**2)**2 + (1-x[i])**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to Rosenbrock Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0               : [-1. -1.]\n",
      "rosenbrock f(w0) : 404.0\n",
      "----------------------------------\n",
      "xk               : [0.98892181 0.97792171]\n",
      "rosenbrock f(xk) : 0.0001229255320492028\n",
      "nsteps           : 8345\n",
      "norm(gradfx)     : 0.009997142295548968\n"
     ]
    }
   ],
   "source": [
    "fx, gradfx = rosenbrock, grad(rosenbrock)\n",
    "x0, alpha, tol, maxiter = np.array([-1.,-1.]), 1e-3, 1e-2, 20000\n",
    "xk, steps = gradient_descent(fx, gradfx, x0, alpha, tol, maxiter)\n",
    "\n",
    "print(\"x0               :\", x0)\n",
    "print(\"rosenbrock f(w0) :\", rosenbrock(x0))\n",
    "print(\"----------------------------------\")\n",
    "print(\"xk               :\", xk)\n",
    "print(\"rosenbrock f(xk) :\", rosenbrock(xk))\n",
    "print(\"nsteps           :\", len(steps))\n",
    "print(\"norm(gradfx)     :\", np.linalg.norm(steps[-1][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

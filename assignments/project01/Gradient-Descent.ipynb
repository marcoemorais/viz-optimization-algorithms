{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "The Gradient Descent method is a first-order iterative optimization algorithm for finding the local minimum of a differentiable function.\n",
    "\n",
    "### Gradient Descent Algorithm\n",
    "1. Start with some initial guess $x_0$ and learning rate $\\alpha$.\n",
    "2. Update $x_k$ in the direction of negative gradient $x_k = x_{k-1} - \\alpha \\nabla f(x_{k-1})$.\n",
    "3. Evaluate the gradient at the new minimum $\\nabla f(x_k)$\n",
    "4. Repeat from step 2 until $\\nabla f(x_k) \\approx 0$\n",
    "\n",
    "### References\n",
    "> Mykel J. Kochenderfer and Tim A. Wheeler. 2019. Algorithms for Optimization. The MIT Press."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The numpy interface of autograd wraps all numpy ops with autodiff.\n",
    "import autograd.numpy as np\n",
    "\n",
    "from autograd import grad\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(fx, gradfx, x0, alpha, tol, maxiter=None):\n",
    "    \"\"\"\n",
    "    gradient_descent returns the point xk where fx is minimum\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fx : function\n",
    "        function to minimize\n",
    "    gradfx : function\n",
    "        gradient of function to minimize\n",
    "    x0 : numpy.ndarray\n",
    "        initial guess for xk\n",
    "    alpha : float\n",
    "        learning rate\n",
    "    tol : float\n",
    "        convergence threshold\n",
    "    maxiter : int\n",
    "        maximum number of iterations\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        point xk where fx is minimum\n",
    "    list\n",
    "        list containing points [(x0, fx(x0), gradfx(x0)), ...])\n",
    "    \"\"\"\n",
    "\n",
    "    xk, fxk, gradfxk = x0, fx(x0), gradfx(x0)\n",
    "    steps = [(x0, fxk, gradfxk)]\n",
    "    \n",
    "    # Stop iteration when gradient is near zero.\n",
    "    while np.linalg.norm(gradfxk) > tol:\n",
    "\n",
    "        # Update xk based on product of learning rate and gradient.\n",
    "        xk = xk - alpha * gradfxk\n",
    "\n",
    "        # Evaluate gradient at new value of xk.\n",
    "        gradfxk = gradfx(xk)\n",
    "\n",
    "        # Evaluate the function at new value of xk.\n",
    "        fxk = fx(xk)\n",
    "\n",
    "        # Append (xk, fxk, gradfxk) to iteration history.\n",
    "        steps.append((xk, fxk, gradfxk))\n",
    "\n",
    "        # Check early termination criteria.\n",
    "        if maxiter is not None and len(steps) == maxiter:\n",
    "            break\n",
    "\n",
    "    return xk, steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Function: Rosenbrock Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x):\n",
    "    \"\"\"\n",
    "    rosenbrock evaluates Rosenbrock function at vector x\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array\n",
    "        x is a D-dimensional vector, [x1, x2, ..., xD]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        scalar result\n",
    "    \"\"\"\n",
    "    D = len(x)\n",
    "    i, iplus1 = np.arange(0,D-1), np.arange(1,D)\n",
    "    return np.sum(100*(x[iplus1] - x[i]**2)**2 + (1-x[i])**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to Rosenbrock Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0               : [-1. -1.]\n",
      "rosenbrock f(w0) : 404.0\n",
      "----------------------------------\n",
      "xk               : [0.98892181 0.97792171]\n",
      "rosenbrock f(xk) : 0.0001229255320492028\n",
      "nsteps           : 8345\n",
      "norm(gradfx)     : 0.009997142295548968\n"
     ]
    }
   ],
   "source": [
    "fx, gradfx = rosenbrock, grad(rosenbrock)\n",
    "x0, alpha, tol, maxiter = np.array([-1.,-1.]), 1e-3, 1e-2, 20000\n",
    "xk, steps = gradient_descent(fx, gradfx, x0, alpha, tol, maxiter)\n",
    "\n",
    "print(\"x0               :\", x0)\n",
    "print(\"rosenbrock f(w0) :\", rosenbrock(x0))\n",
    "print(\"----------------------------------\")\n",
    "print(\"xk               :\", xk)\n",
    "print(\"rosenbrock f(xk) :\", rosenbrock(xk))\n",
    "print(\"nsteps           :\", len(steps))\n",
    "print(\"norm(gradfx)     :\", np.linalg.norm(steps[-1][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Function: Goldstein-Price Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def goldstein_price(x):\n",
    "    \"\"\"\n",
    "    goldstein_price evaluates Goldstein-Price function at vector x\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array\n",
    "        x is a 2-dimensional vector, [x1, x2]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        scalar result\n",
    "    \"\"\"\n",
    "    a = (x[0] + x[1] + 1)**2\n",
    "    b = 19 - 14*x[0] + 3*x[0]**2 - 14*x[1] + 6*x[0]*x[1] + 3*x[1]**2\n",
    "    c = (2*x[0] - 3*x[1])**2\n",
    "    d = 18 - 32*x[0] + 12*x[0]**2 + 48*x[1] - 36*x[0]*x[1] + 27*x[1]**2\n",
    "    return (1. + a*b) * (30. + c*d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to Goldstein-Price Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0                    : [-1.  -1.5]\n",
      "goldstein_price f(w0) : 1595.41015625\n",
      "----------------------------------\n",
      "xk                    : [-2.23924172e-05 -1.00001049e+00]\n",
      "goldstein_price f(xk) : 3.0000001231481543\n",
      "nsteps                : 2419\n",
      "norm(gradfx)          : 0.009960882484498525\n"
     ]
    }
   ],
   "source": [
    "fx, gradfx = goldstein_price, grad(goldstein_price)\n",
    "x0, alpha, tol, maxiter = np.array([-1.0,-1.5]), 1e-5, 1e-2, 20000\n",
    "xk, steps = gradient_descent(fx, gradfx, x0, alpha, tol, maxiter)\n",
    "\n",
    "print(\"x0                    :\", x0)\n",
    "print(\"goldstein_price f(w0) :\", goldstein_price(x0))\n",
    "print(\"----------------------------------\")\n",
    "print(\"xk                    :\", xk)\n",
    "print(\"goldstein_price f(xk) :\", goldstein_price(xk))\n",
    "print(\"nsteps                :\", len(steps))\n",
    "print(\"norm(gradfx)          :\", np.linalg.norm(steps[-1][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
